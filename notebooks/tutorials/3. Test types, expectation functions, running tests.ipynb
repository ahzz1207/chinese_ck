{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.test_types import MFT, INV, DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will assume that our task is sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = Editor(language='chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Functionality Test (MFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Minimum Functionality Test is like a unit test in Software Engineering.\n",
    "If you are testing a certain capability (e.g. 'can the model handle negation?'), an MFT is composed of simple examples that verify a specific behavior.  \n",
    "Let's create a very simple MFT for negations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = ['good', 'enjoyable', 'exciting', 'excellent', 'amazing', 'great', 'engaging']\n",
    "neg = ['bad', 'terrible', 'awful', 'horrible']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some data with both positive and negative negations, assuming `1` means positive and `0` means negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "/root/.cache/torch/transformers/a75f2e45a9463e784dfe8c1d9672440d5fc1b091d5ab104e3c2d82e90ab1b222.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\nSome weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nmodel.init\nNone False tensor([[[ -8.1742,  -8.1554,  -8.0430,  ...,  -7.1040,  -6.7624,  -7.1820],\n         [ -9.8869, -10.0461,  -9.7615,  ...,  -7.2092,  -4.7706,  -6.9981],\n         [-12.0170, -13.0143, -12.3590,  ...,  -8.1703, -10.7526,  -8.1970],\n         ...,\n         [ -4.3623,  -4.2337,  -4.3115,  ...,  -3.3245,  -4.2599,  -3.4409],\n         [ -7.8758,  -8.0636,  -7.8705,  ...,  -2.9529,  -1.0537,  -1.6392],\n         [ -8.9220,  -9.2771,  -9.2942,  ...,  -6.8720,  -6.1491,  -5.5310]]],\n       device='cuda:0')\nNone False tensor([[[ -8.0773,  -8.0539,  -7.9638,  ...,  -7.0572,  -6.7636,  -7.2470],\n         [ -9.7639,  -9.9362,  -9.6745,  ...,  -7.0851,  -4.9011,  -6.9229],\n         [-11.8349, -12.8512, -12.2019,  ...,  -8.0882, -10.6923,  -8.0026],\n         ...,\n         [ -4.1681,  -4.0817,  -4.1945,  ...,  -2.1873,  -2.6228,  -2.6962],\n         [ -8.0062,  -8.3454,  -8.0319,  ...,  -3.0593,  -0.8686,  -1.4598],\n         [ -8.8583,  -9.1916,  -9.2537,  ...,  -6.8875,  -6.3233,  -6.1459]]],\n       device='cuda:0')\nNone False tensor([[[ -8.1445,  -8.1547,  -8.0340,  ...,  -7.1447,  -6.7126,  -7.2471],\n         [ -9.7556,  -9.9233,  -9.6460,  ...,  -7.0725,  -4.6630,  -6.8532],\n         [-11.9679, -12.9067, -12.2242,  ...,  -8.3234, -10.9969,  -8.2771],\n         ...,\n         [ -4.3909,  -4.0999,  -4.0782,  ...,  -2.8154,  -2.7530,  -3.3008],\n         [ -7.9551,  -8.2025,  -7.9704,  ...,  -2.9577,  -0.6386,  -1.3552],\n         [ -8.6992,  -9.0196,  -9.0051,  ...,  -6.5017,  -5.4586,  -5.2249]]],\n       device='cuda:0')\nNone False tensor([[[ -8.1211,  -8.1070,  -8.0619,  ...,  -7.1714,  -6.9107,  -7.3634],\n         [ -9.7420,  -9.9203,  -9.6507,  ...,  -7.0295,  -4.9644,  -6.9252],\n         [-11.6250, -12.5825, -11.9640,  ...,  -7.8448, -10.5936,  -7.4607],\n         ...,\n         [ -4.3958,  -4.2572,  -4.3797,  ...,  -2.3399,  -2.7273,  -3.0528],\n         [ -7.6841,  -8.1625,  -7.7594,  ...,  -2.7834,  -0.5122,  -0.6089],\n         [ -8.8048,  -9.1969,  -9.2652,  ...,  -6.8108,  -5.6408,  -5.6610]]],\n       device='cuda:0')\nNone False tensor([[[ -8.0955,  -8.0518,  -7.9736,  ...,  -7.1068,  -6.7636,  -7.1931],\n         [ -9.7516,  -9.9305,  -9.6524,  ...,  -7.0959,  -4.8568,  -6.8752],\n         [-12.3523, -13.4434, -12.7788,  ...,  -8.4175, -11.5687,  -8.4408],\n         ...,\n         [ -4.4769,  -4.3127,  -4.0914,  ...,  -2.3900,  -2.4063,  -3.1083],\n         [ -7.9762,  -8.1729,  -7.9562,  ...,  -3.1335,  -0.7824,  -1.3179],\n         [ -8.8559,  -9.1173,  -9.2001,  ...,  -6.5935,  -5.9990,  -5.6975]]],\n       device='cuda:0')\nNone False tensor([[[ -8.1826,  -8.1644,  -8.0559,  ...,  -7.1503,  -6.7757,  -7.2523],\n         [ -9.8331,  -9.9858,  -9.7114,  ...,  -7.1369,  -4.7542,  -7.0099],\n         [-11.8880, -12.8290, -12.2231,  ...,  -8.1133, -10.7478,  -8.1174],\n         ...,\n         [ -4.5084,  -4.3195,  -4.4277,  ...,  -3.1033,  -3.5050,  -2.9284],\n         [ -8.2218,  -8.4109,  -8.1572,  ...,  -3.1208,  -0.7857,  -1.7115],\n         [ -8.7130,  -9.0110,  -9.0598,  ...,  -6.8146,  -5.9304,  -5.5850]]],\n       device='cuda:0')\nNone False tensor([[[ -8.1493,  -8.1224,  -8.0426,  ...,  -7.1853,  -6.8355,  -7.2903],\n         [ -9.8179,  -9.9749,  -9.6922,  ...,  -7.0180,  -4.7141,  -6.9702],\n         [-11.6838, -12.6796, -11.9266,  ...,  -7.9233, -10.1470,  -7.8435],\n         ...,\n         [ -4.1624,  -4.3684,  -4.3111,  ...,  -2.2348,  -1.5174,  -2.4387],\n         [ -8.0496,  -8.4611,  -8.1092,  ...,  -2.7531,  -0.3124,  -1.3258],\n         [ -8.8589,  -9.1950,  -9.1818,  ...,  -6.8134,  -5.8952,  -5.8311]]],\n       device='cuda:0')\nNone False tensor([[[ -8.1563,  -8.1313,  -8.0594,  ...,  -7.2170,  -6.8138,  -7.2886],\n         [ -9.8464, -10.0030,  -9.7155,  ...,  -7.0512,  -4.7227,  -7.0476],\n         [-11.7864, -12.7767, -12.0245,  ...,  -7.9796, -10.2233,  -8.0168],\n         ...,\n         [ -4.0822,  -4.3122,  -4.2134,  ...,  -2.4049,  -1.7847,  -2.6346],\n         [ -8.1516,  -8.5354,  -8.2077,  ...,  -2.9959,  -0.5767,  -1.7516],\n         [ -8.8921,  -9.2314,  -9.2029,  ...,  -6.9115,  -5.9679,  -6.1432]]],\n       device='cuda:0')\n"
    }
   ],
   "source": [
    "ret = editor.template('This is not {a:pos} {mask}.', pos=pos, labels=0, save=True, nsamples=100)\n",
    "ret += editor.template('This is not {a:neg} {mask}.', neg=neg, labels=1, save=True, nsamples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily turn this data into an MFT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(ret.data, labels=ret.labels, name='Simple negation',\n",
    "           capability='Negation', description='Very simple negations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `ret` is a dict where keys have the right names for test arguments, we can also use a simpler call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MFT(**ret, name='Simple negation',\n",
    "           capability='Negation', description='Very simple negations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use an off-the-shelf sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict_proba(inputs):\n",
    "    p1 = np.array([(sentiment(x)[0] + 1)/2. for x in inputs]).reshape(-1, 1)\n",
    "    p0 = 1- p1\n",
    "    return np.hstack((p0, p1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.15, 0.85],\n       [0.85, 0.15]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions are random\n",
    "predict_proba(['good', 'bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of running tests.  \n",
    "In the first (and simplest) way, you pass a function as argument to `test.run`, which gets called to make predictions.  \n",
    "We assume that the function returns a tuple with `(predictions, confidences)`, so we have a wrapper to turn softmax (like our function above) into this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(predict_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([1]), array([[0.15, 0.85]]))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_pp(['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have this function, running the test is as simple as calling `test.run`.  \n",
    "You can run the test on a subset of testcases (for speed's sake) by specifying `n` if needed.  \n",
    "We won't do that here since our test is small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Predicting 200 examples\n"
    }
   ],
   "source": [
    "test.run(wrapped_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you run a test, you can print a summary of the results with `test.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Test cases:      200\nFails (rate):    95 (47.5%)\n\nExample fails:\n0.8 This is not an amazing mind.\n----\n0.7 This is not an engaging work.\n----\n1.0 This is not an excellent name.\n----\n"
    }
   ],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that this off-the-shelf system has trouble with negation.\n",
    "Note the failures: examples that should be negative are predicted as positive and vice versa (the number shown is the probability of positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using jupyter notebooks, you can use `test.visual_summary()` for a nice visualization version of these results:  \n",
    "(I'll load a gif so you can see this in preview mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141d5862cb3c490d83c312a9a2fdbb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "TestSummarizer(stats={'npassed': 105, 'nfailed': 95, 'nfiltered': 0}, summarizer={'name': 'Simple negation', '…"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from IPython.display import HTML, Image\n",
    "# with open('visual_summary.gif','rb') as f:\n",
    "#     display(Image(data=f.read(), format='png'))\n",
    "test.visual_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way to run a test is from a prediction file.  \n",
    "First, we export the test into a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_raw_file('/tmp/raw_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not an amazing approach.\r\n",
      "This is not a great film.\r\n",
      "This is not an amazing article.\r\n",
      "This is not an engaging report.\r\n",
      "This is not an enjoyable bet.\r\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/raw_file.txt | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you get predictions from the examples in the raw file (in order) however you want, and save them in a prediction file.  \n",
    "Let's simulate this process here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = open('/tmp/raw_file.txt').read().splitlines()\n",
    "preds = predict_proba(docs)\n",
    "f = open('/tmp/softmax_preds.txt', 'w')\n",
    "for p in preds:\n",
    "    f.write('%f %f\\n' % tuple(p))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.200000 0.800000\r\n",
      "0.700000 0.300000\r\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/softmax_preds.txt | head -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the test from this file.  \n",
    "We have to specify the file format (see the API for possible choices), or a function that takes a line in the file and outputs predictions and confidences.  \n",
    "Since we had already run this test, we have to set `overwrite=True` to overwrite the previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.run_from_file('/tmp/softmax_preds.txt', file_format='softmax', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      200\n",
      "Fails (rate):    94 (47.0%)\n",
      "\n",
      "Example fails:\n",
      "0.8 This is not an amazing approach.\n",
      "----\n",
      "0.7 This is not an engaging look.\n",
      "----\n",
      "0.0 This is not an awful movie.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariance tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Invariance test (INV) is when we apply label-preserving perturbations to inputs and expect the model prediction to remain the same.  \n",
    "Let's start by creating a fictitious dataset to serve as an example, and process it with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ['This was a very nice movie directed by John Smith.',\n",
    "           'Mary Keen was brilliant.', \n",
    "          'I hated everything about this.',\n",
    "          'This movie was very bad.',\n",
    "          'I really liked this movie.',\n",
    "          'just bad.',\n",
    "          'amazing.',\n",
    "          ]\n",
    "pdataset = list(nlp.pipe(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply a simple perturbation: changing people's names and expecting predictions to remain the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was a very nice movie directed by John Smith.\n",
      "This was a very nice movie directed by Michael Morris.\n",
      "This was a very nice movie directed by Christopher Taylor.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "t = Perturb.perturb(pdataset, Perturb.change_names)\n",
    "print('\\n'.join(t.data[0][:3]))\n",
    "print('...')\n",
    "test = INV(**t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 22 examples\n",
      "Test cases:      2\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "test.run(wrapped_pp)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different test: adding typos and expecting predictions to remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was a very nice movie directed by John Smith.\n",
      "This was a very nice movie directed byJ ohn Smith.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "t = Perturb.perturb(dataset, Perturb.add_typos)\n",
    "print('\\n'.join(t.data[0][:3]))\n",
    "print('...')\n",
    "test = INV(**t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 14 examples\n",
      "Test cases:      7\n",
      "Fails (rate):    2 (28.6%)\n",
      "\n",
      "Example fails:\n",
      "0.9 Mary Keen was brilliant.\n",
      "0.5 Mary Keen was brillinat.\n",
      "\n",
      "----\n",
      "0.8 amazing.\n",
      "0.5 amaizng.\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.run(wrapped_pp)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directional Expectation tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Directional Expectation test (DIR) is just like an INV, in the sense that we apply a perturbation to existing inputs. However, instead of expecting invariance, we expect the model to behave in a some specified way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's start with a very simple perturbation: we'll add very negative phrases to the end of our small dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_negative(x):\n",
    "    phrases = ['Anyway, I thought it was bad.', 'Having said this, I hated it', 'The director should be fired.']\n",
    "    return ['%s %s' % (x, p) for p in phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This was a very nice movie directed by John Smith.',\n",
       " ['This was a very nice movie directed by John Smith. Anyway, I thought it was bad.',\n",
       "  'This was a very nice movie directed by John Smith. Having said this, I hated it',\n",
       "  'This was a very nice movie directed by John Smith. The director should be fired.'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0], add_negative(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would we expect after this perturbation? I think the least we should expect is that the prediction probability of positive should **not go up** (that is, it should be monotonically decreasing).  \n",
    "Monotonicity is an expectation function that is built in, so we don't need to implement it.\n",
    "`tolerance=0.1` means we won't consider it a failure if the prediction probability goes up by less than 0.1, only if it goes up by more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.expect import Expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotonic_decreasing = Expect.monotonic(label=1, increasing=False, tolerance=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(dataset, add_negative)\n",
    "test = DIR(**t, expect=monotonic_decreasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 28 examples\n",
      "Test cases:      7\n",
      "After filtering: 6 (85.7%)\n",
      "Fails (rate):    0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "test.run(wrapped_pp)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing custom expectation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are writing a custom expectation functions, it must return a float or bool for each example such that:\n",
    "- `> 0` (or True) means passed,\n",
    "- `<= 0` or False means fail, and (optionally) the magnitude of the failure, indicated by distance from 0, e.g. -10 is worse than -1\n",
    "- `None` means the test does not apply, and this should not be counted\n",
    "\n",
    "Each test case can have multiple examples. In our MFTs, each test case only had a single example, but in our INVs and DIRs, they had multiple examples (e.g. we changed people's names to various other names).\n",
    "\n",
    "You can write custom expectation functions at multiple levels of granularity.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation on a single example\n",
    "\n",
    "If you want to write an expectation function that acts on each individual example, you write a function with the following signature:\n",
    "\n",
    "`def fn(x, pred, conf, label=None, meta=None):`\n",
    "\n",
    "For example, let's write a (useless) expectation function that checks that every prediction confidence is higher than 0.95:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that expects prediction confidence to always be more than 0.9\n",
    "def high_confidence(x, pred, conf, label=None, meta=None):\n",
    "    return conf.max() > 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then wrap this function with `Expect.single`, and apply it to our previous test to see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "expect_fn = Expect.single(high_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      7\n",
      "Fails (rate):    7 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "0.0 I hated everything about this.\n",
      "0.1 I hated everything about this. Anyway, I thought it was bad.\n",
      "0.0 I hated everything about this. Having said this, I hated it\n",
      "\n",
      "----\n",
      "0.8 amazing.\n",
      "0.5 amazing. Anyway, I thought it was bad.\n",
      "0.4 amazing. Having said this, I hated it\n",
      "\n",
      "----\n",
      "0.2 just bad.\n",
      "0.2 just bad. Anyway, I thought it was bad.\n",
      "0.1 just bad. Having said this, I hated it\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test.set_expect(expect_fn)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every test case fails now: there is always some prediction in it that has confidence smaller than 0.95.  \n",
    "By default, the way we aggregate all results in a test case is such that the testcase fails if **any** examples in it fail (for MFTs), or **any but the first** fail for INVs and DIRs (because the first is usually the original data point before perturbation). You can change these defaults with the `agg_fn` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expectation on  pairs \n",
    "\n",
    "Most of the time for DIRs, you want to write an expectation function that acts on pairs of `(original, new)` examples - that is, the original example and the perturbed examples. If this is the case, the signature is as follows:\n",
    "\n",
    "`def fn(orig_pred, pred, orig_conf, conf, labels=None, meta=None)`\n",
    "\n",
    "For example, let's write an expectation function that checks that the prediction **changed** after applying the perturbation, and wrap it with `Expect.pairwise`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changed_pred(orig_pred, pred, orig_conf, conf, labels=None, meta=None):\n",
    "    return pred != orig_pred\n",
    "expect_fn = Expect.pairwise(changed_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually create a new test where we add negation to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This was a very nice movie directed by John Smith.',\n",
       "  'This was not a very nice movie directed by John Smith.'],\n",
       " ['Mary Keen was brilliant.', 'Mary Keen was not brilliant.']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Perturb.perturb(pdataset, Perturb.add_negation)\n",
    "t.data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 10 examples\n",
      "Test cases:      5\n",
      "Fails (rate):    1 (20.0%)\n",
      "\n",
      "Example fails:\n",
      "0.8 I really liked this movie.\n",
      "0.6 I really didn't like this movie.\n",
      "\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "test = DIR(**t, expect=expect_fn)\n",
    "test.run(wrapped_pp)\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the failure: prediction did not change after adding negation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write much more complex expectation functions, but these are enough for this tutorial.  \n",
    "You can check out `expect.py` or the notebooks for Sentiment Analysis, QQP and SQuAD for many additional examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit8e6945bbe1b44598a616c749bfba4850"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}