{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bit8e6945bbe1b44598a616c749bfba4850",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import pkuseg\n",
    "from checklist.model_api import test_model\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.expect import Expect\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.pred_wrapper import PredictorWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Building prefix dict from the default dictionary ...\n08/24/2020 20:36:44 - DEBUG - jieba -   Building prefix dict from the default dictionary ...\nLoading model from cache /tmp/jieba.cache\n08/24/2020 20:36:44 - DEBUG - jieba -   Loading model from cache /tmp/jieba.cache\nLoading model cost 0.879 seconds.\n08/24/2020 20:36:45 - DEBUG - jieba -   Loading model cost 0.879 seconds.\nPrefix dict has been built successfully.\n08/24/2020 20:36:45 - DEBUG - jieba -   Prefix dict has been built successfully.\n08/24/2020 20:36:50 - INFO - transformers.tokenization_utils_base -   Model name '/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model' is a path, a model identifier, or url to a directory containing tokenizer files.\n08/24/2020 20:36:50 - INFO - transformers.tokenization_utils_base -   Didn't find file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/added_tokens.json. We won't load it.\n08/24/2020 20:36:50 - INFO - transformers.tokenization_utils_base -   Didn't find file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/special_tokens_map.json. We won't load it.\n08/24/2020 20:36:50 - INFO - transformers.tokenization_utils_base -   Didn't find file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/tokenizer_config.json. We won't load it.\n08/24/2020 20:36:50 - INFO - transformers.tokenization_utils_base -   Didn't find file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/tokenizer.json. We won't load it.\n08/24/2020 20:36:50 - INFO - transformers.tokenization_utils_base -   loading file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/vocab.txt\n08/24/2020 20:36:50 - INFO - transformers.tokenization_utils_base -   loading file None\n08/24/2020 20:36:50 - INFO - transformers.tokenization_utils_base -   loading file None\n08/24/2020 20:36:50 - INFO - transformers.tokenization_utils_base -   loading file None\n08/24/2020 20:36:50 - INFO - transformers.tokenization_utils_base -   loading file None\n08/24/2020 20:36:50 - INFO - transformers.configuration_utils -   loading configuration file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/config.json\n08/24/2020 20:36:50 - INFO - transformers.configuration_utils -   Model config BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"directionality\": \"bidi\",\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 21128\n}\n\n08/24/2020 20:36:50 - INFO - transformers.modeling_utils -   loading weights file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/pytorch_model.bin\n/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/pytorch_model.bin\n08/24/2020 20:36:54 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n08/24/2020 20:36:54 - WARNING - transformers.modeling_utils -   Some weights of BertForMaskedLM were not initialized from the model checkpoint at /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model and are newly initialized: ['cls.predictions.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nmodel.init\n"
    },
    {
     "data": {
      "text/plain": "<checklist.text_generation.TextGenerator at 0x7f275b489ac8>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = checklist.editor.Editor()\n",
    "seg = pkuseg.pkuseg()\n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "suite = TestSuite()\n",
    "editor.tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a9bd6d5ba414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabel2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/data/intent/new_label2id_680.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/data/intent/dev_680_train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "examples, labels, texts = [], [], []\n",
    "label2id = json.load(open(\"/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/data/intent/new_label2id_680.json\"))\n",
    "for line in open(\"/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/data/intent/dev_680_train.txt\").readlines()[:1000]:\n",
    "    line = line.strip('\\n')\n",
    "    lines = line.split('|')\n",
    "    texts.append(line)\n",
    "    examples.append(lines[0])\n",
    "    labels.append(label2id[lines[1]])\n",
    "labels = np.array(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[对硬盘稳定性要求高, 西湖里面一般有啥吃的空格贵吗]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_examples = list(nlp.pipe(examples))\n",
    "spacy_map = dict([(x, y) for x, y in zip(examples, parsed_examples)])\n",
    "parsed_qs = [(spacy_map[q]) for q in examples]\n",
    "parsed_qs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "找, 说, 黑, 问, 骂, 叫, 学, 请, 夸, 嫁, 用, 谈, 求, 和, 跟, 靠, 娶, 看, 买, 做\n"
    }
   ],
   "source": [
    "template = \"我是要{v:mask}张伟。他是工程师\"\n",
    "verbs = editor.suggest(templates=template, )[:20]\n",
    "print(', '.join(verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = editor.template('我是要{verb}张伟,他是工程师',\n",
    "                verb=verbs,\n",
    "                remove_duplicates=True, \n",
    "                nsamples=20)\n",
    "test = MFT(**t, labels=563, name='动词测试', capability='Vocabulary', \n",
    "          description = '填充不同动词，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['我是要找张伟,她是招聘经理', '我是要找张伟,她是南中国区高科技及生命科学主管合伙人', '我是要找张伟,她是排版设计师']\n"
    }
   ],
   "source": [
    "t = editor.template('我是要找张伟,她是{post}',\n",
    "                remove_duplicates=True, \n",
    "                nsamples=100)\n",
    "print(t.data[:3])\n",
    "test = MFT(**t, labels=563, name='职位测试', capability='Vocabulary', \n",
    "          description = '填充不同职位，测试分类是否依然准确')\n",
    "suite.add(test, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ['真的', '确实', '绝对', '毫无疑问', '确确实实']\n",
    "t = editor.template('我{mod}不是{company}的员工!', mod=mod, remove_duplicates=True, nsamples=50)\n",
    "test = MFT(**t, labels=361, name='修饰语测试', capability='Vocabulary', \n",
    "          description = '增加不同的修饰语，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = editor.suggest('我好{a:mask}啊，去休息室怎么走？')[:20]\n",
    "t = editor.template('我好{adj}啊，去休息室怎么走？', adj=adj, remove_duplicates=True, nsamples=50)\n",
    "test = MFT(**t, labels=495, name='形容词测试', capability='Vocabulary', \n",
    "          description = '使用不同的形容词，测试分类是否依然准确')\n",
    "suite.add(test, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "followup = ['会伤身吗？', '会伤肾吗?','伤身体吗？', '可以一直吃吗?']\n",
    "t = editor.template('蛋白粉吃多了会怎么样?{followup}', followup=followup, remove_duplicates=True, nsamples=20)\n",
    "test = MFT(**t, labels=523, name='增加追问', capability='Vocabulary', \n",
    "          description = '增加一段追问，测试分类是否依然准确')\n",
    "suite.add(test, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2b2601f83c21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'什么空气净化器不太{mask}?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'什么空气净化器%s?'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/editor.py\u001b[0m in \u001b[0;36msynonyms\u001b[0;34m(self, templates, word, threshold, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \"\"\"\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wordnet_stuff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'synonyms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrelated_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/editor.py\u001b[0m in \u001b[0;36m_wordnet_stuff\u001b[0;34m(self, templates, word, type, threshold, depth, pos, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m         }[type]\n\u001b[1;32m    352\u001b[0m         \u001b[0;31m# return [x[0][0] for x in fn(texts, word, threshold=threshold, pos=pos, depth=depth)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mantonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/text_generation.py\u001b[0m in \u001b[0;36msynonyms\u001b[0;34m(self, texts, word, threshold, pos, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msynonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# options = all_possible_synonyms(word, pos=pos)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_most_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# options = ['热的', '很热']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/text_generation.py\u001b[0m in \u001b[0;36mget_most_sim\u001b[0;34m(word, http)\u001b[0m\n\u001b[1;32m     48\u001b[0m                             data={\"word\": [word], \"n\": ['10']})\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "syn = []\n",
    "x = editor.suggest('什么空气净化器比较{mask}?')\n",
    "x += editor.suggest('什么空气净化器不太{mask}?')\n",
    "for a in set(x):\n",
    "    e = editor.synonyms('什么空气净化器%s?' % a, a)\n",
    "    if e:\n",
    "        syn.append([a] + e)\n",
    "print(',\\n'.join([str(tuple(x)) for x in tmp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = []\n",
    "for a in set(x):\n",
    "    e = editor.antonyms('什么空气净化器%s?' % a, a)\n",
    "    if e:\n",
    "        ops.append([a] + e)\n",
    "print(',\\n'.join([str(tuple(x)) for x in tmp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [y for x in syn for y in x] + [y for x in ops for y in x]\n",
    "data = editor.template('什么空气净化器{syn}?', syn=temp, remove_duplicates=True, nsamples=50)\n",
    "test = MFT(**t, labels=655, name='测试同义词和反义词', capability='Taxonomy', \n",
    "          description = '填充同义词或反义词，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[什么空气净化器流行, 看看十二月的传统型彩票的中奖号码]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = []\n",
    "for line in open(\"/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/test_classify/classify.txt\").readlines()[:10]:\n",
    "    line = line.strip('\\n')\n",
    "    examples.append(line)\n",
    "parsed_examples = list(nlp.pipe(examples))\n",
    "spacy_map = dict([(x, y) for x, y in zip(examples, parsed_examples)])\n",
    "parsed_qs = [(spacy_map[q]) for q in examples]\n",
    "parsed_qs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "for e in examples:\n",
    "    syns = []\n",
    "    es = seg.cut(e)\n",
    "    for word in es:\n",
    "        for s in editor.synonyms(e, word)[1]:\n",
    "            synonyms.append((e, s))\n",
    "        for s in editor.antonyms(e, word)[1]:\n",
    "            antonyms.append((e, s))\n",
    "        # syns.append(editor.synonyms(e, word))\n",
    "    # synonyms.append(zip(es, syns))\n",
    "synonyms[:2]\n",
    "antonyms[:2]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def replace_pairs(pairs):\n",
    "    def replace_z(text):\n",
    "        ret = []\n",
    "        for x, y in pairs:\n",
    "            t = text.replace(x, y)\n",
    "            if t != text:\n",
    "                ret.append(t)\n",
    "            t = text.replace(y, x)\n",
    "            if t != text:\n",
    "                ret.append(t)\n",
    "        return list(set(ret))\n",
    "    return replace_z\n",
    "def apply_and_pair(fn):\n",
    "    def ret_fn(text):\n",
    "        ret = fn(text)\n",
    "        return [(text, r) for r in ret]\n",
    "    return ret_fn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(list(examples), apply_and_pair(replace_pairs(synonyms)), nsamples=100, keep_original=False)\n",
    "test = INV(t.data, threshold=0.1, name='同义词替换', description='替换近义词，分类是否会受影响', capability='Taxonomy')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(list(examples), apply_and_pair(replace_pairs(antonyms)), nsamples=100, keep_original=False)\n",
    "test = INV(t.data, threshold=0.1, name='反义词替换', description='替换反义词，分类是否会受影响', capability='Taxonomy')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_fn(fn):\n",
    "    def real_fn(text):\n",
    "        ret = fn(text)\n",
    "        if ret == text:\n",
    "            return \n",
    "        return [(text, ret)]\n",
    "    return real_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['什么空气净化器流行', '看看十二月的传统型彩票的中奖号码', '冰箱是使用什么来除臭的']\n"
    }
   ],
   "source": [
    "t = Perturb.perturb(examples, wrapper_fn(Perturb.add_typos), nsamples=100)\n",
    "test = INV(t.data, name='随机替换', capability='Robustness', description='对文本随机替换1个字符，分类结果应尽量不变')\n",
    "suite.add(test, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conctraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(examples, wrapper_fn(Perturb.contractions), nsamples=100)\n",
    "test = INV(**t, name='缩写转换', capability='Robustness', description='对文本中的缩写简写进行转换，分类结果应保持不变')\n",
    "suite.add(test, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改变句子中的姓名，数字，地点等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 姓名替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "male = random.choices(editor.data['names']['male'], 1000)\n",
    "t = editor.template('我是要找{male},他是工程师',\n",
    "                male=male,\n",
    "                remove_duplicates=True, \n",
    "                nsamples=100)\n",
    "test = MFT(**t, labels=563, name='姓名测试', capability='NER', \n",
    "          description = '填充不同男性姓名，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female = random.choices(editor.data['names']['female'], 1000)\n",
    "t = editor.template('我是要找{female},她是工程师',\n",
    "                male=female,\n",
    "                remove_duplicates=True, \n",
    "                nsamples=100)\n",
    "test = MFT(**t, labels=563, name='姓名测试', capability='NER', \n",
    "          description = '填充不同女性姓名，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = editor.template('我不是{company}的员工!', remove_duplicates=True, nsamples=50)\n",
    "test = MFT(**t, labels=361, name='公司测试', capability='NER', \n",
    "          description = '填充不同公司，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_change(fn):\n",
    "    def apply_change(text):\n",
    "        seed = np.random.randint(100)\n",
    "        c = fn(text, seed=seed, meta=False)\n",
    "        if not c:\n",
    "            return\n",
    "        if type(c) == list:\n",
    "            return [(text, c1) for c1 in c]\n",
    "        elif c == text:\n",
    "            return\n",
    "        else: \n",
    "            return [(text, c)]\n",
    "    return apply_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_examples, wrapper_change(Perturb.change_names), nsamples=100)\n",
    "test = INV(**t, name='姓名替换', capability='NER',\n",
    "          description='对所有句子中的姓名随机替换')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_qs, wrapper_change(Perturb.trans_num), nsamples=100)\n",
    "test = INV(**t, name='数字替换', capability='NER',\n",
    "          description='对所有句子中的小写数字转大写数字')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(parsed_qs, wrapper_change(Perturb.change_location), nsamples=100)\n",
    "test = INV(**t, name='地点替换', capability='NER',\n",
    "          description='对所有句子中的城市，国家，地名进行对应替换')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实体随机填充，替换句式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = editor.template('我不是{male1}，我是{company}的{post}，我叫{male2}。', remove_duplicates=True, nsamples=100)\n",
    "data2 = editor.template('我叫{male2}，我不是{male1}，我是{company}的{post}', remove_duplicates=True, nsamples=100)\n",
    "data = zip(data1, data2)\n",
    "test = INV(data, name='句式替换', capability='Temporal',\n",
    "          description='两种句式是否结果一致')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 颠倒句子顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_list(text):\n",
    "    splits = text.split('，')\n",
    "    if len(splits) > 1:\n",
    "        n = random.choice(range(len(splits)))\n",
    "        swap = splits[n]\n",
    "        y = random.choice(range(len(splits)).remove(n))\n",
    "        splits[n] = splits[y]\n",
    "        splits[y] = swap\n",
    "        return [(text, '，'.join(splits))]\n",
    "    else:\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(examples, change_list, keep_original=False, nsamples=100)\n",
    "test = INV(**t, name='顺序替换', capability='Temporal',\n",
    "          description='两种句式是否结果一致')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(examples, wrapper_change(Perturb.add_negation), nsamples=100)\n",
    "test = INV(t.data, name='肯定否定替换', capability='Negation', description='对文本中肯定转否定，否定转肯定')\n",
    "suite.add(test, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "males = zip(male[:100], female[:100])\n",
    "data = []\n",
    "for m, f in males:\n",
    "    data.append((f\"我的名字是{m}，而不是{f}。\", f\"我的名字是{f}，而不是{m}。\"))\n",
    "test = INV(data, name='名字肯否定替换', capability='Negation', description='名字转换，句式相同，句意改变')\n",
    "suite.add(test, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}