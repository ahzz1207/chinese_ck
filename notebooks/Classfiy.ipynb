{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bit8e6945bbe1b44598a616c749bfba4850",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from checklist.model_api import test_model\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.expect import Expect\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.pred_wrapper import PredictorWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Building prefix dict from the default dictionary ...\n08/23/2020 23:07:09 - DEBUG - jieba -   Building prefix dict from the default dictionary ...\nLoading model from cache /tmp/jieba.cache\n08/23/2020 23:07:09 - DEBUG - jieba -   Loading model from cache /tmp/jieba.cache\nLoading model cost 0.874 seconds.\n08/23/2020 23:07:10 - DEBUG - jieba -   Loading model cost 0.874 seconds.\nPrefix dict has been built successfully.\n08/23/2020 23:07:10 - DEBUG - jieba -   Prefix dict has been built successfully.\n08/23/2020 23:07:16 - INFO - transformers.tokenization_utils_base -   Model name '/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model' is a path, a model identifier, or url to a directory containing tokenizer files.\n08/23/2020 23:07:16 - INFO - transformers.tokenization_utils_base -   Didn't find file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/added_tokens.json. We won't load it.\n08/23/2020 23:07:16 - INFO - transformers.tokenization_utils_base -   Didn't find file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/special_tokens_map.json. We won't load it.\n08/23/2020 23:07:16 - INFO - transformers.tokenization_utils_base -   Didn't find file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/tokenizer_config.json. We won't load it.\n08/23/2020 23:07:16 - INFO - transformers.tokenization_utils_base -   Didn't find file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/tokenizer.json. We won't load it.\n08/23/2020 23:07:16 - INFO - transformers.tokenization_utils_base -   loading file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/vocab.txt\n08/23/2020 23:07:16 - INFO - transformers.tokenization_utils_base -   loading file None\n08/23/2020 23:07:16 - INFO - transformers.tokenization_utils_base -   loading file None\n08/23/2020 23:07:16 - INFO - transformers.tokenization_utils_base -   loading file None\n08/23/2020 23:07:16 - INFO - transformers.tokenization_utils_base -   loading file None\n08/23/2020 23:07:16 - INFO - transformers.configuration_utils -   loading configuration file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/config.json\n08/23/2020 23:07:16 - INFO - transformers.configuration_utils -   Model config BertConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"directionality\": \"bidi\",\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 21128\n}\n\n08/23/2020 23:07:16 - INFO - transformers.modeling_utils -   loading weights file /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/pytorch_model.bin\n/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model/pytorch_model.bin\n08/23/2020 23:07:29 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n08/23/2020 23:07:29 - WARNING - transformers.modeling_utils -   Some weights of BertForMaskedLM were not initialized from the model checkpoint at /work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/checklist/checklist-master/checklist/data/model and are newly initialized: ['cls.predictions.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nmodel.init\n"
    },
    {
     "data": {
      "text/plain": "<checklist.text_generation.TextGenerator at 0x7f1b01acaef0>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = checklist.editor.Editor()\n",
    "nlp = spacy.load('zh_core_web_sm')\n",
    "editor.tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "examples, labels, texts = [], [], []\n",
    "label2id = json.load(open(\"/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/data/intent/new_label2id_680.json\"))\n",
    "for line in open(\"/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/data/intent/dev_680_train.txt\").readlines()[:10]:\n",
    "    line = line.strip('\\n')\n",
    "    lines = line.split('|')\n",
    "    texts.append(line)\n",
    "    examples.append(lines[0])\n",
    "    labels.append(label2id[lines[1]])\n",
    "labels = np.array(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[对硬盘稳定性要求高, 西湖里面一般有啥吃的空格贵吗]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_examples = list(nlp.pipe(examples))\n",
    "spacy_map = dict([(x, y) for x, y in zip(examples, parsed_examples)])\n",
    "suite = TestSuite()\n",
    "parsed_qs = [(spacy_map[q]) for q in examples]\n",
    "parsed_qs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "找, 说, 黑, 问, 骂, 叫, 学, 请, 夸, 嫁\n"
    }
   ],
   "source": [
    "template = \"我是要{v:mask}张伟。他是工程师\"\n",
    "verbs = editor.suggest(templates=template, )[:10]\n",
    "print(', '.join(verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "male = editor.data['names']['male']\n",
    "data = editor.template('我是要{verb}{male},他是工程师',\n",
    "                verb=verbs,\n",
    "                male=male,\n",
    "                remove_duplicates=True, \n",
    "                nsamples=100)\n",
    "data[:3]\n",
    "test = MFT(**t, labels=563, name='动词测试', capability='Vocabulary', \n",
    "          description = '填充不同动词，名字填充为男性，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female = editor.data['names']['female']\n",
    "data = editor.template('我是要找{female},她是{post}',\n",
    "                female=female,\n",
    "                remove_duplicates=True, \n",
    "                nsamples=100)\n",
    "data[:3]\n",
    "test = MFT(**t, labels=563, name='职位测试', capability='Vocabulary', \n",
    "          description = '填充不同职位，名字填充为女性，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = editor.template('我不是{company}的员工!', remove_duplicates=True, nsamples=50)\n",
    "test = MFT(**t, labels=361, name='公司测试', capability='Vocabulary', \n",
    "          description = '填充不同公司，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ['真的', '确实', '绝对', '毫无疑问', '确确实实']\n",
    "data = editor.template('我{mod}不是{company}的员工!', mod=mod, remove_duplicates=True, nsamples=50)\n",
    "test = MFT(**t, labels=361, name='修饰语测试', capability='Vocabulary', \n",
    "          description = '增加不同的修饰语，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = editor.suggest('我好{a:mask}啊，去休息室怎么走？')[:20]\n",
    "data = editor.template('我好{adj}啊，去休息室怎么走？', adj=adj, remove_duplicates=True, nsamples=50)\n",
    "test = MFT(**t, labels=495, name='形容词测试', capability='Vocabulary', \n",
    "          description = '使用不同的形容词，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followup = ['会伤身吗？', '会伤肾吗?','伤身体吗？', '可以一直吃吗?']\n",
    "data = editor.template('蛋白粉吃多了会怎么样?{followup}', followup=followup, remove_duplicates=True, nsamples=20)\n",
    "test = MFT(**t, labels=523, name='增加追问', capability='Vocabulary', \n",
    "          description = '增加一段追问，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'editor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9f24f086c7c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'什么空气净化器比较{mask}?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'什么空气净化器不太{mask}?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meditor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'什么空气净化器{moreless} %s?'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoreless\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'more'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'less'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'editor' is not defined"
     ]
    }
   ],
   "source": [
    "syn = []\n",
    "x = editor.suggest('什么空气净化器比较{mask}?')\n",
    "x += editor.suggest('什么空气净化器不太{mask}?')\n",
    "for a in set(x):\n",
    "    e = editor.synonyms('什么空气净化器%s?' % a, a)\n",
    "    if e:\n",
    "        syn.append([a] + e)\n",
    "print(',\\n'.join([str(tuple(x)) for x in tmp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = []\n",
    "for a in set(x):\n",
    "    e = editor.antonyms('什么空气净化器%s?' % a, a)\n",
    "    if e:\n",
    "        ops.append([a] + e)\n",
    "print(',\\n'.join([str(tuple(x)) for x in tmp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [y for x in syn for y in x] + [y for x in ops for y in x]\n",
    "data = editor.template('什么空气净化器{syn}?', syn=temp, remove_duplicates=True, nsamples=50)\n",
    "test = MFT(**t, labels=655, name='测试同义词和反义词', capability='Taxonomy', \n",
    "          description = '填充同义词或反义词，测试分类是否依然准确')\n",
    "suite.add(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Perturb.perturb(list(all_questions), apply_and_pair(replace_pairs(synonyms)), nsamples=1000, keep_original=False)\n",
    "test = INV(t.data, threshold=0.1, name=name, description=desc, capability='Taxonomy')"
   ]
  }
 ]
}