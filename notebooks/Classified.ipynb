{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bit8e6945bbe1b44598a616c749bfba4850",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "08/13/2020 21:23:36 - INFO - transformers.file_utils -   PyTorch version 1.3.0+cu100 available.\n08/13/2020 21:23:39 - INFO - transformers.file_utils -   TensorFlow version 2.0.0 available.\n"
    }
   ],
   "source": [
    "import checklist\n",
    "import spacy\n",
    "import itertools\n",
    "from checklist.model_api import circle_model\n",
    "import checklist.editor\n",
    "import checklist.text_generation\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.expect import Expect\n",
    "from checklist.test_suite import TestSuite\n",
    "import numpy as np\n",
    "import spacy\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.pred_wrapper import PredictorWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = circle_model()\n",
    "wrapped_pp = PredictorWrapper.wrap_softmax(test_model.eval)\n",
    "editor = checklist.editor.Editor(language='chinese')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试分类模型对POS+Vocabulary的能力，构造MFT INV DIR三种测试方案\n",
    "首先构造MFT测试\n",
    "我们首先利用词库提供一些地名，构造一些简单样本进行分类准确率的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['去蚂蚁金服是地铁几号线,我该怎么去', '去腾讯是地铁几号线,我该怎么去', '去特斯联是地铁几号线,我该怎么去', '去海康威视是地铁几号线,我该怎么去', '去小马智行是地铁几号线,我该怎么去']\n"
    }
   ],
   "source": [
    "ret = editor.template('去{company}是地铁几号线,我该怎么去', labels=468, nsamples=200)\n",
    "ret += editor.template('坐什么公交可以到{company}', labels=467, nsamples=200)\n",
    "print(ret.data[:5])\n",
    "test = MFT(ret.data, labels=ret.labels, name='vocabluary',\n",
    "           capability='vocab', description='test model vocab ')\n",
    "suite.add(test)\n",
    "# test.run(wrapped_pp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造INV测试, 使用模型生成一些动词，利用这些动词对样本随机替换\n",
    "模型应该保持预测结果不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_v(data):\n",
    "    if data.find('去') > -1:\n",
    "        new_data=data.replace('去', random.choice(v))\n",
    "    else:\n",
    "        new_data=data.replace('坐', random.choice(v2))\n",
    "    return [new_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['去蚂蚁金服是地铁几号线,我该怎么去', '去腾讯是地铁几号线,我该怎么去', '去特斯联是地铁几号线,我该怎么去']\n08/13/2020 21:25:29 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n08/13/2020 21:25:30 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /root/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.f12a4f986e43d8b328f5b067a641064d67b91597567a06c7b122d1ca7dfd9741\n08/13/2020 21:25:30 - INFO - transformers.configuration_utils -   Model config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"directionality\": \"bidi\",\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"pooler_fc_size\": 768,\n  \"pooler_num_attention_heads\": 12,\n  \"pooler_num_fc_layers\": 3,\n  \"pooler_size_per_head\": 128,\n  \"pooler_type\": \"first_token_transform\",\n  \"type_vocab_size\": 2,\n  \"vocab_size\": 21128\n}\n\n08/13/2020 21:25:31 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-chinese-pytorch_model.bin from cache at /root/.cache/torch/transformers/a75f2e45a9463e784dfe8c1d9672440d5fc1b091d5ab104e3c2d82e90ab1b222.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n/root/.cache/torch/transformers/a75f2e45a9463e784dfe8c1d9672440d5fc1b091d5ab104e3c2d82e90ab1b222.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n08/13/2020 21:25:38 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n08/13/2020 21:25:38 - WARNING - transformers.modeling_utils -   Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['cls.predictions.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nmodel.init\n"
    }
   ],
   "source": [
    "# 这里复用mft单元生成的数据，由于INV测试只比较模型是否对扰动前后样本预测结果相同，因此INV数据不需要label标签\n",
    "data = ret.data\n",
    "print(data[:3])\n",
    "v = editor.suggest('去{company}是地铁几号线？我应该怎么{mask}呢？')[:20]\n",
    "v2 = editor.suggest('我应该{mask}什么公交可以到{company}')[:20]\n",
    "t = Perturb.perturb(data, change_v)\n",
    "inv = INV(**t, name='verb test', capability='vocab')\n",
    "suite.add(inv)\n",
    "# 将测试数据重新写入文件中\n",
    "# examples = inv.to_raw_file('/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/data/checklist/dev_468_inv.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后生成DIR测试，我们希望肯定转为否定能降低模型对类别判断的置信度(概率)\n",
    "我们首先需要编写一个自定义的期望函数，即认为概率下降/上升多少该case为pass/fail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们期望采取否定语法后，所有样本不改变预测结果\n",
    "def high_confidence(x, pred, conf, label=None, meta=None):\n",
    "    return conf.max() < 0.6\n",
    "expect_fn = Expect.single(high_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['去蚂蚁金服是地铁几号线,我该怎么去', '去蚂蚁金服别是地铁几号线,我该怎么去'], ['去腾讯是地铁几号线,我该怎么去', '去腾讯并不是地铁几号线,我该怎么去'], ['去特斯联是地铁几号线,我该怎么去', '去特斯联非地铁几号线,我该怎么去']]\n"
    }
   ],
   "source": [
    "# 继续沿用INV的data\n",
    "t = Perturb.perturb(data, Perturb.add_negation)\n",
    "print(t.data[:3])\n",
    "dir = DIR(**t, expect=expect_fn, name=\"negation test\", capability='vocab')\n",
    "suite.add(dir)\n",
    "# 将测试数据重新写入文件中\n",
    "# examples = inv.to_raw_file('/work/QA_task/roberta-1.1/BERTCN/bertcn-pytorch-r1.1/data/checklist/dev_468_dir.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "08/13/2020 21:25:39 - INFO - checklist.model_api -   ***** Running devaluation *****\n08/13/2020 21:25:39 - INFO - checklist.model_api -     Num examples = 400\n08/13/2020 21:25:39 - INFO - root -   Start dev!\nIteration:   1%|          | 4/400 [00:00<00:10, 37.90it/s]Running vocabluary\nPredicting 400 examples\nIteration: 100%|██████████| 400/400 [00:09<00:00, 41.38it/s]\n08/13/2020 21:25:48 - INFO - root -   Start dev!\nIteration: 100%|██████████| 400/400 [00:09<00:00, 43.17it/s]\n08/13/2020 21:25:58 - INFO - checklist.model_api -   ***** Running devaluation *****\n08/13/2020 21:25:58 - INFO - checklist.model_api -     Num examples = 800\n08/13/2020 21:25:58 - INFO - root -   Start dev!\nIteration:   0%|          | 0/800 [00:00<?, ?it/s]本轮eval耗时共18.99524760246277\nRunning verb test\nPredicting 800 examples\nIteration: 100%|██████████| 800/800 [00:19<00:00, 41.99it/s]\n08/13/2020 21:26:17 - INFO - root -   Start dev!\nIteration: 100%|██████████| 800/800 [00:19<00:00, 41.24it/s]\n08/13/2020 21:26:36 - INFO - checklist.model_api -   ***** Running devaluation *****\n08/13/2020 21:26:36 - INFO - checklist.model_api -     Num examples = 800\n08/13/2020 21:26:36 - INFO - root -   Start dev!\nIteration:   0%|          | 0/800 [00:00<?, ?it/s]本轮eval耗时共38.57008361816406\nRunning negation test\nPredicting 800 examples\nIteration: 100%|██████████| 800/800 [00:19<00:00, 41.90it/s]\n08/13/2020 21:26:56 - INFO - root -   Start dev!\nIteration: 100%|██████████| 800/800 [00:19<00:00, 41.62it/s]本轮eval耗时共38.43224763870239\n\n"
    }
   ],
   "source": [
    "suite.run(wrapped_pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "vocab\n\nvocabluary\nTest cases:      400\nFails (rate):    5 (1.2%)\n\nExample fails:\n383 (0.5) 坐什么公交可以到大众点评\n----\n383 (0.5) 坐什么公交可以到大众点评\n----\n383 (0.5) 坐什么公交可以到大众点评\n----\n\n\nverb test\nTest cases:      400\nFails (rate):    48 (12.0%)\n\nExample fails:\n468 (0.5) 去蚂蚁金服是地铁几号线,我该怎么去\n194 (0.3) 做蚂蚁金服是地铁几号线,我该怎么做\n\n----\n467 (0.4) 坐什么公交可以到第四范式\n41 (0.3) 问什么公交可以到第四范式\n\n----\n467 (0.6) 坐什么公交可以到头条\n121 (0.4) 换什么公交可以到头条\n\n----\n\n\nnegation test\nTest cases:      400\nFails (rate):    50 (12.5%)\n\nExample fails:\n467 (0.5) 坐什么公交可以到影谱科技\n467 (0.6) 坐什么公交别到影谱科技\n\n----\n467 (0.6) 坐什么公交可以到旷视科技\n467 (0.7) 坐什么公交别到旷视科技\n\n----\n467 (0.6) 坐什么公交可以到中科曙光\n467 (0.7) 坐什么公交无法到中科曙光\n\n----\n\n\n\n\n"
    }
   ],
   "source": [
    "suite.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Please wait as we prepare the table data...\n"
    }
   ],
   "source": [
    "a, b = suite.visual_summary_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53e727e416b491ab4e63625cbdc8054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "SuiteSummarizer(stats={'npassed': 0, 'nfailed': 0, 'nfiltered': 0}, test_infos=[{'name': 'vocabluary', 'descri…"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from checklist.viewer.suite_summarizer import SuiteSummarizer\n",
    "SuiteSummarizer(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}